# -*- coding: utf-8 -*-
"""Final_RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WKgKkoKk1C8cpT3PeG_Yh_ussTuWrOKb
"""

# =========================
# STEP 1: IMPORT LIBRARIES
# =========================
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.layers import (
    Input, Dense, LayerNormalization,
    MultiHeadAttention, LSTM, GlobalAveragePooling1D
)
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping

# =========================
# STEP 2: LOAD DATA
# =========================
df = pd.read_csv(
    "household_power_consumption.csv",
    sep=",",
    na_values="?"
)

df["Datetime"] = pd.to_datetime(df["time"], errors="coerce")
df.drop(columns=["time"], inplace=True)
df.set_index("Datetime", inplace=True)

df = df.astype(float)
df.fillna(method="ffill", inplace=True)

features = [
    "Global_active_power",
    "Global_reactive_power",
    "Voltage",
    "Global_intensity",
    "Sub_metering_1",
    "Sub_metering_2",
    "Sub_metering_3"
]

data = df[features]

# =========================
# STEP 3: DATA REDUCTION (PERFORMANCE)
# =========================
MAX_SAMPLES = 100_000
data = data.tail(MAX_SAMPLES)

# =========================
# STEP 4: SCALING
# =========================
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

# =========================
# STEP 5: SEQUENCE CREATION
# =========================
def create_sequences(data, seq_len=48):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len, 0])
    return np.array(X), np.array(y)

SEQ_LEN = 48
X, y = create_sequences(scaled_data, SEQ_LEN)

# =========================
# STEP 6: ROLLING WINDOW CV
# =========================
def rolling_cv(X, y, train_size=0.7, step=20000):
    folds = []
    train_end = int(len(X) * train_size)
    while train_end + step < len(X):
        folds.append((
            X[:train_end], y[:train_end],
            X[train_end:train_end+step], y[train_end:train_end+step]
        ))
        train_end += step
    return folds

folds = rolling_cv(X, y)

# =========================
# STEP 7: METRICS
# =========================
def smape(y_true, y_pred):
    return np.mean(
        2 * np.abs(y_pred - y_true) /
        (np.abs(y_true) + np.abs(y_pred) + 1e-8)
    )

def mase(y_true, y_pred):
    naive_forecast = y_true[:-1]
    naive_actual = y_true[1:]
    return np.mean(np.abs(y_true - y_pred)) / np.mean(np.abs(naive_actual - naive_forecast))

# =========================
# STEP 8: TRANSFORMER MODEL
# =========================
def transformer_model(input_shape):
    inputs = Input(shape=input_shape)

    attn = MultiHeadAttention(num_heads=4, key_dim=32)(inputs, inputs)
    x = LayerNormalization()(inputs + attn)

    ffn = Dense(128, activation="relu")(x)
    ffn = Dense(x.shape[-1])(ffn)
    x = LayerNormalization()(x + ffn)

    x = GlobalAveragePooling1D()(x)
    outputs = Dense(1)(x)

    model = Model(inputs, outputs)
    model.compile(optimizer="adam", loss="mse")
    return model

# =========================
# STEP 9: TRAIN & EVALUATE (TRANSFORMER)
# =========================
rmse_scores, smape_scores, mase_scores = [], [], []
best_rmse = np.inf

for fold, (X_train, y_train, X_test, y_test) in enumerate(folds):
    print(f"Transformer Fold {fold+1}")

    model = transformer_model((X_train.shape[1], X_train.shape[2]))

    model.fit(
        X_train,
        y_train,
        epochs=8,
        batch_size=64,
        validation_split=0.2,
        callbacks=[EarlyStopping(patience=2, restore_best_weights=True)],
        verbose=0
    )

    preds = model.predict(X_test).flatten()
    rmse = np.sqrt(mean_squared_error(y_test, preds))

    rmse_scores.append(rmse)
    smape_scores.append(smape(y_test, preds))
    mase_scores.append(mase(y_test, preds))

    if rmse < best_rmse:
        best_rmse = rmse
        model.save("best_transformer_model.h5")

print("\nTransformer Performance (CV Average)")
print("RMSE :", np.mean(rmse_scores))
print("sMAPE:", np.mean(smape_scores))
print("MASE :", np.mean(mase_scores))

# =========================
# STEP 10: BASELINE LSTM (SAME CV)
# =========================
lstm_rmse, lstm_smape, lstm_mase = [], [], []

for fold, (X_train, y_train, X_test, y_test) in enumerate(folds):
    print(f"LSTM Fold {fold+1}")

    lstm = tf.keras.Sequential([
        LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),
        Dense(1)
    ])

    lstm.compile(optimizer="adam", loss="mse")
    lstm.fit(X_train, y_train, epochs=6, batch_size=64, verbose=0)

    preds = lstm.predict(X_test).flatten()

    lstm_rmse.append(np.sqrt(mean_squared_error(y_test, preds)))
    lstm_smape.append(smape(y_test, preds))
    lstm_mase.append(mase(y_test, preds))

# =========================
# STEP 1: IMPORT LIBRARIES
# =========================
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.layers import (
    Input, Dense, LayerNormalization,
    MultiHeadAttention, LSTM, GlobalAveragePooling1D
)
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping

# =========================
# STEP 2: LOAD & CLEAN DATA
# =========================
df = pd.read_csv(
    "household_power_consumption.csv",
    sep=",",
    na_values="?"
)

# Standardize column names
df.columns = df.columns.str.strip().str.lower()

# Datetime handling
df["datetime"] = pd.to_datetime(df["time"], errors="coerce")
df.drop(columns=["time"], inplace=True)
df.set_index("datetime", inplace=True)

# Convert to numeric & fill missing values
df = df.astype(float)
df = df.ffill()

# =========================
# STEP 3: FEATURE SELECTION
# =========================
features = [
    "global_active_power",
    "global_reactive_power",
    "voltage",
    "global_intensity",
    "sub_metering_1",
    "sub_metering_2",
    "sub_metering_3"
]

data = df[features]

# =========================
# STEP 4: DATA REDUCTION (PERFORMANCE)
# =========================
MAX_SAMPLES = 100_000
data = data.tail(MAX_SAMPLES)

# =========================
# STEP 5: SCALING
# =========================
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

# =========================
# STEP 6: SEQUENCE CREATION
# =========================
def create_sequences(data, seq_len=48):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len, 0])
    return np.array(X), np.array(y)

SEQ_LEN = 48
X, y = create_sequences(scaled_data, SEQ_LEN)

# =========================
# STEP 7: ROLLING WINDOW CV
# =========================
def rolling_cv(X, y, train_size=0.7, step=20000):
    folds = []
    train_end = int(len(X) * train_size)
    while train_end + step < len(X):
        folds.append((
            X[:train_end], y[:train_end],
            X[train_end:train_end+step], y[train_end:train_end+step]
        ))
        train_end += step
    return folds

folds = rolling_cv(X, y)

# =========================
# STEP 8: METRICS
# =========================
def smape(y_true, y_pred):
    return np.mean(
        2 * np.abs(y_pred - y_true) /
        (np.abs(y_true) + np.abs(y_pred) + 1e-8)
    )

def mase(y_true, y_pred):
    naive_forecast = y_true[:-1]
    naive_actual = y_true[1:]
    return np.mean(np.abs(y_true - y_pred)) / np.mean(np.abs(naive_actual - naive_forecast))

# =========================
# STEP 9: TRANSFORMER MODEL
# =========================
def transformer_model(input_shape):
    inputs = Input(shape=input_shape)

    # Self-attention block
    attn = MultiHeadAttention(num_heads=4, key_dim=32)(inputs, inputs)
    x = LayerNormalization()(inputs + attn)

    # Feed-forward block
    ffn = Dense(128, activation="relu")(x)
    ffn = Dense(x.shape[-1])(ffn)
    x = LayerNormalization()(x + ffn)

    x = GlobalAveragePooling1D()(x)
    outputs = Dense(1)(x)

    model = Model(inputs, outputs)
    model.compile(optimizer="adam", loss="mse")
    return model

# =========================
# STEP 10: TRAIN & EVALUATE â€“ TRANSFORMER
# =========================
rmse_scores, smape_scores, mase_scores = [], [], []
best_rmse = np.inf

for fold, (X_train, y_train, X_test, y_test) in enumerate(folds):
    print(f"Transformer Fold {fold + 1}")

    model = transformer_model((X_train.shape[1], X_train.shape[2]))

    model.fit(
        X_train,
        y_train,
        epochs=8,
        batch_size=64,
        validation_split=0.2,
        callbacks=[EarlyStopping(patience=2, restore_best_weights=True)],
        verbose=0
    )

    preds = model.predict(X_test).flatten()
    rmse = np.sqrt(mean_squared_error(y_test, preds))

    rmse_scores.append(rmse)
    smape_scores.append(smape(y_test, preds))
    mase_scores.append(mase(y_test, preds))

    if rmse < best_rmse:
        best_rmse = rmse
        model.save("best_transformer_model.h5")

print("\nTransformer Performance (CV Average)")
print("RMSE :", np.mean(rmse_scores))
print("sMAPE:", np.mean(smape_scores))
print("MASE :", np.mean(mase_scores))

# =========================
# STEP 11: BASELINE LSTM (SAME CV)
# =========================
lstm_rmse, lstm_smape, lstm_mase = [], [], []

for fold, (X_train, y_train, X_test, y_test) in enumerate(folds):
    print(f"LSTM Fold {fold + 1}")

    lstm = tf.keras.Sequential([
        LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),
        Dense(1)
    ])

    lstm.compile(optimizer="adam", loss="mse")
    lstm.fit(X_train, y_train, epochs=6, batch_size=64, verbose=0)

    preds = lstm.predict(X_test).flatten()

    lstm_rmse.append(np.sqrt(mean_squared_error(y_test, preds)))
    lstm_smape.append(smape(y_test, preds))
    lstm_mase.append(mase(y_test, preds))

print("\nLSTM Performance (CV Average)")
print("RMSE :", np.mean(lstm_rmse))
print("sMAPE:", np.mean(lstm_smape))
print("MASE :", np.mean(lstm_mase))